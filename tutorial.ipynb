{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from lc_macro_pipeline.retiler import Retiler\n",
    "from lc_macro_pipeline.data_processing import DataProcessing\n",
    "from lc_macro_pipeline.geotiff_writer import Geotiff_writer\n",
    "from lc_macro_pipeline.macro_pipeline import MacroPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-ecology LiDAR point-cloud processing pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking whether the test data set is available locally, we otherwise retrieve it from the AHN3 repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_dir = 'testdata'\n",
    "testdata_files = ['C_41CZ2.LAZ']\n",
    "\n",
    "file_paths = [Path(testdata_dir).joinpath(f) for f in testdata_files]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if not file_path.is_file():\n",
    "        file_url = '/'.join(['https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_laz', \n",
    "                                      file_path])\n",
    "        urlretrieve(file_url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files generate by the pipeline will be saved in the `temp_folder` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = Path('/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retiling\n",
    "\n",
    "The first step in the pipeline is to retile the retrieved point-cloud files to a regular grid, splitting the original data into smaller chuncks that are easier to handle for data processing. The boundaries of the grid and the number of tiles along each axis are set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'min_x': -113107.8100,\n",
    "    'max_x': 398892.1900,\n",
    "    'min_y': 214783.8700,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retiling of multiple input files consists of independent tasks, which are thus efficiently parallelized. The input controlling all the steps of the retiling is organized in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retiling_macro = MacroPipeline()\n",
    "\n",
    "#TODO: setup the dask cluster/client\n",
    "# from dask.distributed import Client, SetupMyCluster\n",
    "# cluster = SetupMyCluster()\n",
    "# client = Client(cluster)\n",
    "# retiling_macro.set_client(client)\n",
    "\n",
    "for file_path in file_paths:\n",
    "    retiler = Retiler()\n",
    "    retiler.input = {\n",
    "        'localfs': {\n",
    "            'input_folder': file_path.parent.as_posix(),\n",
    "            'input_file': file_path.name,\n",
    "            'temp_folder': temp_folder\n",
    "        },\n",
    "        'tiling': grid,\n",
    "        'split_and_redistribute': {},\n",
    "        'validate': {}\n",
    "    }\n",
    "    retiling_macro.add_task(retiler)\n",
    "\n",
    "res = retiling_macro.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "Once the files are splitted into tiles of a manageable size, we proceed to the feature extraction stage. The base input dictionary for this step looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_input_base = {\n",
    "    \"normalize\": {\n",
    "        \"cell_size\": 1\n",
    "    },\n",
    "    \"generate_targets\": {\n",
    "        'min_x': -113107.8100,\n",
    "        'max_x': 398892.1900,\n",
    "        'min_y': 214783.8700,\n",
    "        'max_y': 726783.87,\n",
    "        'n_tiles_side': 256,\n",
    "        \"tile_mesh_size\" : 10.0,\n",
    "        \"validate\" : True,\n",
    "    },\n",
    "    \"extract_features\": {\n",
    "        \"feature_names\": [\"coeff_var_normalized_height\"],\n",
    "        \"volume_type\": \"cell\",\n",
    "        \"volume_size\": 10\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tiles to which the original input file has been retiled are listed in a record file located in the temporary directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = []\n",
    "for file_path in file_paths:\n",
    "    record_file = '_'.join([file_path.stem, 'retile_record.js'])\n",
    "    with Path(temp_folder/file_path.stem/record_file).open() as f:\n",
    "        record = json.load(f)\n",
    "    assert record['validated']\n",
    "    tiles += [Path(temp_folder/file_path.stem/tile)\n",
    "              for tile in record['redistributed_to']]\n",
    "print([t.as_posix() for t in tiles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tile can be processed independently, so that again one can run the tasks in a parallel fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_macro = MacroPipeline()\n",
    "# data_processing_macro.set_client(client)\n",
    "\n",
    "for tile in [tiles[0]]:\n",
    "    dp_input = dp_input_base.copy()\n",
    "    dp_input.update({\n",
    "        'load': {'path': tile.as_posix()},\n",
    "        'export_targets': {'path': tile.with_suffix('.PLY').as_posix()}\n",
    "    })\n",
    "    dp_input['generate_targets'].update({'index_tile_x': int(tile.name.split('_')[1]), \n",
    "                                         'index_tile_y': int(tile.name.split('_')[2])})\n",
    "    dp = DataProcessing()\n",
    "    dp.input = dp_input\n",
    "    dp_macro.add_task(dp)\n",
    "\n",
    "res = dp_macro.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
