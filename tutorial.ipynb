{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from lc_macro_pipeline import Retiler, DataProcessing, GeotiffWriter\n",
    "from lc_macro_pipeline import MacroPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-ecology LiDAR point-cloud processing pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Retrieval  and Cluster Setup\n",
    "\n",
    "Files produced by the pipeline will be saved in the `tmp_folder` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_folder = pathlib.Path('/var/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking whether the test data set is available locally, we otherwise retrieve it from the AHN3 repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_files = ['C_41CZ2.LAZ']\n",
    "\n",
    "file_paths = [tmp_folder/f for f in testdata_files]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if not file_path.is_file():\n",
    "        url = 'https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_laz'\n",
    "        url = '/'.join([url, file_path.name])\n",
    "        urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then setup the cluster that we will use for the computation using `dask`. For this example, the cluster consists of 3 processes. Note: it is important that single-threaded workers are employed for the tasks that require `laserchicken`!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, \n",
    "                       n_workers=2, \n",
    "                       threads_per_worker=1, \n",
    "                       local_directory=tmp_folder/'dask-worker-space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retiling\n",
    "\n",
    "The first step in the pipeline is to retile the retrieved point-cloud files to a regular grid, splitting the original data into smaller chuncks that are easier to handle for data processing. The boundaries of the grid and the number of tiles along each axis are set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'min_x': -113107.8100,\n",
    "    'max_x': 398892.1900,\n",
    "    'min_y': 214783.8700,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retiling of multiple input files consists of independent tasks, which are thus efficiently parallelized. The input controlling all the steps of the retiling is organized in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path where output will be written \n",
    "retiling_out_path = tmp_folder/'retiled'\n",
    "\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {'output_folder': retiling_out_path},\n",
    "    'set_grid': grid,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001 C_41CZ2                        Completed\n"
     ]
    }
   ],
   "source": [
    "retiling_macro = MacroPipeline()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    retiler = Retiler(input_file=file_path, label=file_path.stem)\n",
    "    retiler.config(retiling_input)\n",
    "    retiling_macro.add_task(retiler)\n",
    "\n",
    "retiling_macro.setup_client(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "retiling_macro.run()\n",
    "retiling_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "Once the files are splitted into tiles of a manageable size, we proceed to the feature extraction stage, which is performed using `laserchicken`. We choose the following two example features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['mean_normalized_height', 'std_normalized_height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base input dictionary for this step looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path where output will be written \n",
    "dp_out_path = tmp_folder/'targets'\n",
    "\n",
    "dp_input = {\n",
    "    'setup_local_fs': {'output_folder': dp_out_path},\n",
    "    'load': {},\n",
    "    'normalize': {\n",
    "        'cell_size': 1\n",
    "    },\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : 10.0,\n",
    "        'validate' : True,\n",
    "        **grid\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': feature_names,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': 10\n",
    "    },\n",
    "    'export_targets': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tiles to which the original input file has been retiled are listed in a record file located in the retiling output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/var/tmp/retiled/tile_169_107', '/var/tmp/retiled/tile_169_108', '/var/tmp/retiled/tile_169_106', '/var/tmp/retiled/tile_171_108', '/var/tmp/retiled/tile_170_108', '/var/tmp/retiled/tile_171_107', '/var/tmp/retiled/tile_170_107']\n"
     ]
    }
   ],
   "source": [
    "tiles = []\n",
    "for file_path in file_paths:\n",
    "    record_file = '_'.join([file_path.stem, 'retile_record.js'])\n",
    "    with pathlib.Path(retiling_out_path/record_file).open() as f:\n",
    "        record = json.load(f)\n",
    "    assert record['validated']\n",
    "    tiles += [pathlib.Path(retiling_out_path/tile)\n",
    "              for tile in record['redistributed_to']]\n",
    "print([t.as_posix() for t in tiles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tile can be processed independently, so that again one can run the tasks in a parallel fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001 tile_169_107                   Completed\n",
      "002 tile_169_108                   Completed\n",
      "003 tile_169_106                   Completed\n",
      "004 tile_171_108                   Completed\n",
      "005 tile_170_108                   Completed\n",
      "006 tile_171_107                   Completed\n",
      "007 tile_170_107                   Completed\n"
     ]
    }
   ],
   "source": [
    "dp_macro = MacroPipeline()\n",
    "\n",
    "for tile in tiles:\n",
    "    # parse tile index from the directory name\n",
    "    tile_index = [int(n) for n in tile.name.split('_')[1:]]\n",
    "    dp = DataProcessing(input=tile, label=tile.name, tile_index=tile_index)\n",
    "    dp.config(dp_input)\n",
    "    dp_macro.add_task(dp)\n",
    "    \n",
    "dp_macro.setup_client(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "dp_macro.run()\n",
    "dp_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GeoTIFF Export\n",
    "\n",
    "The last step of the pipeline is the transformation of the features extracted from the point-cloud data and 'rasterized' in the target grid to a GeoTIFF file. In this case, the construction of the geotiffs (one per feature) can be performed in parallel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path where output will be written \n",
    "gw_out_path = tmp_folder/'geotiffs'\n",
    "\n",
    "gw_input = {\n",
    "    'setup_local_fs': {'input_folder': dp_out_path,\n",
    "                       'output_folder': gw_out_path},\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': [1, 1],\n",
    "    'create_subregion_geotiffs': {'output_handle': 'geotiff'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001 mean_normalized_height         Completed\n",
      "002 std_normalized_height          Completed\n"
     ]
    }
   ],
   "source": [
    "geotiff_macro = MacroPipeline()\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    gw = GeotiffWriter(bands=feature_name, label=feature_name)\n",
    "    gw.config(gw_input)\n",
    "    geotiff_macro.add_task(gw)\n",
    "\n",
    "geotiff_macro.setup_client(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "geotiff_macro.run()\n",
    "geotiff_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we stop the client and the scheduler of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
