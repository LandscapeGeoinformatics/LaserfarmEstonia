{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json \n",
    "\n",
    "from distutils import dir_util\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from lc_macro_pipeline.retiler import Retiler\n",
    "from laserchicken.io.load import load\n",
    "from laserchicken import filter\n",
    "from lc_macro_pipeline.data_processing import DataProcessing\n",
    "from lc_macro_pipeline.geotiff_writer import Geotiff_writer\n",
    "from lc_macro_pipeline.macro_pipeline import MacroPipeline\n",
    "from lc_macro_pipeline.classification import Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-ecology LiDAR point-cloud processing pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Retrieval  and Cluster Setup\n",
    "\n",
    "Files produced by the pipeline will be saved in the `temp_folder` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = Path('/var/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking whether the test data set is available locally, we otherwise retrieve it from the AHN3 repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_files = ['C_41CZ2.LAZ']\n",
    "\n",
    "file_paths = [temp_folder.joinpath(f) for f in testdata_files]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if not file_path.is_file():\n",
    "        file_url = '/'.join(['https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_laz', \n",
    "                             file_path.name])\n",
    "        urlretrieve(file_url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then setup the cluster that we will use for the computation using `dask`. For this example, the cluster consists of 3 processes. Note: it is important that single-threaded workers are employed for the tasks that require `laserchicken`!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, \n",
    "                       n_workers=2, \n",
    "                       threads_per_worker=1, \n",
    "                       local_directory=temp_folder.joinpath('dask-worker-space'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retiling\n",
    "\n",
    "The first step in the pipeline is to retile the retrieved point-cloud files to a regular grid, splitting the original data into smaller chuncks that are easier to handle for data processing. The boundaries of the grid and the number of tiles along each axis are set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'min_x': -113107.8100,\n",
    "    'max_x': 398892.1900,\n",
    "    'min_y': 214783.8700,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retiling of multiple input files consists of independent tasks, which are thus efficiently parallelized. The input controlling all the steps of the retiling is organized in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retiling_macro = MacroPipeline()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(file_path)\n",
    "    \n",
    "    retiler = Retiler()\n",
    "    \n",
    "    retiler.input = {\n",
    "        'localfs': {\n",
    "            'input_folder': file_path.parent.as_posix(),\n",
    "            'input_file': file_path.name,\n",
    "            'output_folder': temp_folder\n",
    "        },\n",
    "        'tiling': grid,\n",
    "        'split_and_redistribute': {},\n",
    "        'validate': {}\n",
    "    }\n",
    "    \n",
    "    retiling_macro.add_task(retiler)\n",
    "\n",
    "retiling_macro.setup_client(cluster=cluster)\n",
    "res = retiling_macro.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "Once the files are splitted into tiles of a manageable size, we proceed to the feature extraction stage, which is performed using `laserchicken`. We choose the following two example features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"mean_normalized_height\", \"std_normalized_height\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base input dictionary for this step looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_input = {\n",
    "    \"normalize\": {\n",
    "        \"cell_size\": 1\n",
    "    },\n",
    "    \"generate_targets\": {\n",
    "        'min_x': -113107.8100,\n",
    "        'max_x': 398892.1900,\n",
    "        'min_y': 214783.8700,\n",
    "        'max_y': 726783.87,\n",
    "        'n_tiles_side': 256,\n",
    "        \"tile_mesh_size\" : 10.0,\n",
    "        \"validate\" : True,\n",
    "    },\n",
    "    \"extract_features\": {\n",
    "        \"feature_names\": feature_names,\n",
    "        \"volume_type\": \"cell\",\n",
    "        \"volume_size\": 10\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tiles to which the original input file has been retiled are listed in a record file located in the temporary directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = []\n",
    "for file_path in file_paths:\n",
    "    record_file = '_'.join([file_path.stem, 'retile_record.js'])\n",
    "    with Path(temp_folder/record_file).open() as f:\n",
    "        record = json.load(f)\n",
    "    assert record['validated']\n",
    "    tiles += [Path(temp_folder/tile)\n",
    "              for tile in record['redistributed_to']]\n",
    "print([t.as_posix() for t in tiles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tile can be processed independently, so that again one can run the tasks in a parallel fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_macro = MacroPipeline()\n",
    "\n",
    "for tile in tiles:\n",
    "    print(tile)\n",
    "    \n",
    "    dp = DataProcessing()\n",
    "\n",
    "    # add tile-specific input to the dictionary\n",
    "    dp_input['localfs'] = {'input_folder': tile.as_posix(), 'output_folder': tile.parent.as_posix()}\n",
    "    dp_input['load'] = {}\n",
    "    dp_input['export_targets'] = {'filename': tile.with_suffix('.ply').name, 'overwrite': True}\n",
    "    dp_input['generate_targets']['index_tile_x'] = int(tile.name.split('_')[1]) \n",
    "    dp_input['generate_targets']['index_tile_y'] = int(tile.name.split('_')[2])\n",
    "    dp_input['log_config'] = {'filename': '{}_data_processing.log'.format(tile.name)}\n",
    "    dp.input = copy.deepcopy(dp_input)\n",
    "    \n",
    "    dp_macro.add_task(dp)\n",
    "    \n",
    "dp_macro.setup_client(cluster=cluster)\n",
    "res = dp_macro.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification of target pointset (optional)\n",
    "We can classify the target point cloud according to their groud type, based on given cadaster data. \n",
    "To mark the types of the points in the point cloud, we can add a new column `ground_type` to the target point cloud. We can use the class code of TOP10NL as the identifier. \n",
    "\n",
    "0. Unclassified\n",
    "1. Gebouw\n",
    "2. Inrichtingselement\n",
    "3. Terrein (Polygon)\n",
    "4. Spoorbaandeel\n",
    "5. Waterdeel\n",
    "6. GeografischGebied (Point)\n",
    "7. FunctioneelGebied\n",
    "8. Plaats\n",
    "9. RegistratiefGebied\n",
    "10. Hoogte\n",
    "11. Relief (Line String)\n",
    "12. Wegdeel\n",
    "\n",
    "Here we present an example of classifying points falls on waterbody with given shp files of waterbody polygon. Stored in `./testdata/shp/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we copy the shp file from `./testdata/shp/` in this repository to `/var/tmp/shp/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_util.copy_tree('./testdata/shp/', (temp_folder/'shp').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify the target points in `tile_170_107.ply` accoding to shape files stored in `/var/tmp/shp/`.\n",
    "\n",
    "The pipeline we automatically find out the relavant shp file (`41W_waterdeel.shp` in this case).\n",
    "\n",
    "We will add a new column `ground_type`, and mark all points which fall in the waterbody polygon `5`, which means `waterdeel`.\n",
    "\n",
    "We set up the input for classification pipeline as follow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_input = {\n",
    "    \"localfs\":{\n",
    "        \"input_folder\": temp_folder.as_posix(),\n",
    "        \"output_folder\":(temp_folder/\"classified_target_point\").as_posix()\n",
    "    },\n",
    "    \"locate_shp\":{\n",
    "        \"shp_dir\":\"shp\"\n",
    "    },\n",
    "    \"classification\":{\n",
    "        \"ground_type\":5\n",
    "    },\n",
    "  \"export_point_cloud\":{\n",
    "    \"overwrite\":True\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accelerate the processing, we only apply classification to the points with `mean_normalized_height` larger than 0.2\n",
    "\n",
    "Then we excute the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "target_tiles = ['tile_170_107.ply', 'tile_170_108.ply']\n",
    "\n",
    "cl_macro = MacroPipeline()\n",
    "\n",
    "for tile in target_tiles:\n",
    "    print(tile)\n",
    "    \n",
    "    # Load target point\n",
    "    f_pc = classification_input['localfs']['input_folder']+'/'+tile\n",
    "    pc = load(f_pc)\n",
    "    pc = filter.select_above(pc, 'mean_normalized_height', 0.2) # To accelerate, only use points with mean_normalized_height>0.2\n",
    "\n",
    "    cl = Classification()\n",
    "    # add tile-specific input\n",
    "    classification_input['locate_shp']['point_cloud'] = pc\n",
    "    classification_input['export_point_cloud']['filename'] = Path(tile).stem+'_classification.ply' \n",
    "    cl.input = copy.deepcopy(classification_input)\n",
    "    \n",
    "    cl_macro.add_task(cl)\n",
    "\n",
    "cl_macro.setup_client(cluster=cluster)\n",
    "res = cl_macro.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GeoTIFF Export\n",
    "\n",
    "The last step of the pipeline is the transformation of the features extracted from the point-cloud data and 'rasterized' in the target grid to a GeoTIFF file. In this case, the construction of the geotiffs (one per feature) can be performed in parallel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_macro = MacroPipeline()\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    print(feature_name)\n",
    "    \n",
    "    gw = Geotiff_writer()\n",
    "    gw.input = {\n",
    "        \"log_config\": {'filename': '{}_geotiff_writer.log'.format(feature_name)},\n",
    "        \"localfs\": {'input_folder': temp_folder.as_posix(), 'output_folder': (temp_folder/'geotiff').as_posix()},\n",
    "        \"parse_point_cloud\" : {},\n",
    "        \"data_split\": {\"xSub\": 1, \"ySub\": 1},\n",
    "        \"create_subregion_geotiffs\": {\n",
    "            \"outputhandle\": \"geotiff\",\n",
    "            \"band_export\": [feature_name]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    geotiff_macro.add_task(gw)\n",
    "\n",
    "geotiff_macro.setup_client(cluster=cluster)\n",
    "res = geotiff_macro.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we stop the client and the scheduler of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
